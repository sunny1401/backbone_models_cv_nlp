{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c2698c15",
   "metadata": {},
   "source": [
    "# import io\n",
    "# import os.path\n",
    "# import re\n",
    "# import tarfile\n",
    "\n",
    "# import smart_open\n",
    "\n",
    "# def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "#     with smart_open.open(url, \"rb\") as file:\n",
    "#         with tarfile.open(fileobj=file) as tar:\n",
    "#             for member in tar.getmembers():\n",
    "#                 if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
    "#                     member_bytes = tar.extractfile(member).read()\n",
    "#                     yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "# docs = list(extract_documents())\n",
    "# import pandas as pd\n",
    "# doc_series = pd.DataFrame(docs, columns=[\"raw_text\"])\n",
    "# doc_series.to_csv(\"temp_data_save.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e07844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acbfdcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from src.nlp.preprocessing import DataPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c359752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "doc_series = pd.read_csv(\"temp_data_save.csv\")\n",
    "# doc_series[\"cleaned_results\"] = doc_series.raw_text.apply(lambda sentence: prepare_training_df(sentence))\n",
    "# doc_series[\"title\"] = doc_series.cleaned_results.apply(lambda d: d[1])\n",
    "# doc_series[\"raw_text\"] = doc_series.cleaned_results.apply(lambda d: d[0])\n",
    "# doc_series[\"cleaned_results\"] = doc_series.cleaned_results.apply(lambda d: f\"{d[1]} {d[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b273fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataPreprocessor(doc_series.raw_text, n_grams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92dd0d72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_processor.corpus[\"lemmatized_sentence\"] = data_processor.corpus.cleaned.apply(lambda d: data_processor.lemmatize_text(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5322862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.corpus[\"tokens\"] = data_processor.corpus.lemmatized_sentence.apply(lambda d: data_processor.generate_ngrams(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ede3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.corpus[\"tokens\"] = data_processor.corpus.tokens.apply(lambda d: [\" \".join(token) for token in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9603ba33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO : built Dictionary<3417369 unique tokens: ['able', 'able do', 'able do so', 'able form', 'able form complex']...> from 1740 documents (total 7317807 corpus positions)\n",
      "INFO : Dictionary lifecycle event {'msg': \"built Dictionary<3417369 unique tokens: ['able', 'able do', 'able do so', 'able form', 'able form complex']...> from 1740 documents (total 7317807 corpus positions)\", 'datetime': '2022-12-16T12:04:54.379981', 'gensim': '4.2.0', 'python': '3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "word_dict = Dictionary(data_processor.corpus.tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29420e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 3405486 tokens: [('able do', 14), ('able do so', 6), ('able form', 7), ('able form complex', 1), ('above', 987), ('above examples', 5), ('above examples suggest', 1), ('above methods', 4), ('above methods used', 1), ('above previous', 2)]...\n",
      "INFO : keeping 11883 tokens which were in no less than 20 and no more than 870 (=50.0%) documents\n",
      "INFO : resulting dictionary: Dictionary<11883 unique tokens: ['able', 'adapting', 'addition', 'adjust', 'adjust weights']...>\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "word_dict.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad9a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_path = \"temp_dict_trigrams.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed66f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Dictionary lifecycle event {'fname_or_handle': 'temp_dict_trigrams.txt', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-12-16T12:05:09.736506', 'gensim': '4.2.0', 'python': '3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'saving'}\n",
      "INFO : saved temp_dict_trigrams.txt\n"
     ]
    }
   ],
   "source": [
    "word_dict.save(dictionary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a9b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d545da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "data_processor.corpus[\"embeddings\"] = data_processor.corpus.tokens.apply(word_dict.doc2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5e7908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>lemmatized_sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>387 \\nNeural Net and Traditional Classifiers ...</td>\n",
       "      <td>387 Neural Net and Traditional Classifiers Wil...</td>\n",
       "      <td>neural net traditional classifiers abstract pr...</td>\n",
       "      <td>[neural, net, traditional, classifiers, abstra...</td>\n",
       "      <td>[(0, 2), (1, 1), (2, 2), (3, 1), (4, 1), (5, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 \\nCONNECTIVITY VERSUS ENTROPY \\nYaser S. Abu...</td>\n",
       "      <td>1 CONNECTIVITY VERSUS ENTROPY Yaser S Abu Most...</td>\n",
       "      <td>connectivity entropy abstract connectivity neu...</td>\n",
       "      <td>[connectivity, entropy, abstract, connectivity...</td>\n",
       "      <td>[(12, 1), (14, 1), (15, 1), (24, 1), (30, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9 \\nStochastic Learning Networks and their Ele...</td>\n",
       "      <td>9 Stochastic Learning Networks and their Elect...</td>\n",
       "      <td>stochastic learning networks electronic implem...</td>\n",
       "      <td>[stochastic, learning, networks, electronic, i...</td>\n",
       "      <td>[(2, 4), (3, 2), (12, 1), (15, 1), (17, 3), (2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22 \\nLEARNING ON A GENERAL NETWORK \\nAmir F. A...</td>\n",
       "      <td>22 LEARNING ON A GENERAL NETWORK Amir F Atiya ...</td>\n",
       "      <td>learning general paper generalizes backpropaga...</td>\n",
       "      <td>[learning, general, paper, generalizes, backpr...</td>\n",
       "      <td>[(3, 1), (13, 1), (20, 1), (24, 1), (27, 1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31 \\nAN ARTIFICIAL NEURAL NETWORK FOR SPATIO- ...</td>\n",
       "      <td>31 AN ARTIFICIAL NEURAL NETWORK FOR SPATIO TEM...</td>\n",
       "      <td>artificial neural network spatio temporal bipo...</td>\n",
       "      <td>[artificial, neural, network, spatio, temporal...</td>\n",
       "      <td>[(2, 1), (11, 2), (19, 2), (20, 2), (22, 2), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  \\\n",
       "0  387 \\nNeural Net and Traditional Classifiers ...   \n",
       "1  1 \\nCONNECTIVITY VERSUS ENTROPY \\nYaser S. Abu...   \n",
       "2  9 \\nStochastic Learning Networks and their Ele...   \n",
       "3  22 \\nLEARNING ON A GENERAL NETWORK \\nAmir F. A...   \n",
       "4  31 \\nAN ARTIFICIAL NEURAL NETWORK FOR SPATIO- ...   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  387 Neural Net and Traditional Classifiers Wil...   \n",
       "1  1 CONNECTIVITY VERSUS ENTROPY Yaser S Abu Most...   \n",
       "2  9 Stochastic Learning Networks and their Elect...   \n",
       "3  22 LEARNING ON A GENERAL NETWORK Amir F Atiya ...   \n",
       "4  31 AN ARTIFICIAL NEURAL NETWORK FOR SPATIO TEM...   \n",
       "\n",
       "                                 lemmatized_sentence  \\\n",
       "0  neural net traditional classifiers abstract pr...   \n",
       "1  connectivity entropy abstract connectivity neu...   \n",
       "2  stochastic learning networks electronic implem...   \n",
       "3  learning general paper generalizes backpropaga...   \n",
       "4  artificial neural network spatio temporal bipo...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [neural, net, traditional, classifiers, abstra...   \n",
       "1  [connectivity, entropy, abstract, connectivity...   \n",
       "2  [stochastic, learning, networks, electronic, i...   \n",
       "3  [learning, general, paper, generalizes, backpr...   \n",
       "4  [artificial, neural, network, spatio, temporal...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [(0, 2), (1, 1), (2, 2), (3, 1), (4, 1), (5, 3...  \n",
       "1  [(12, 1), (14, 1), (15, 1), (24, 1), (30, 1), ...  \n",
       "2  [(2, 4), (3, 2), (12, 1), (15, 1), (17, 3), (2...  \n",
       "3  [(3, 1), (13, 1), (20, 1), (24, 1), (27, 1), (...  \n",
       "4  [(2, 1), (11, 2), (19, 2), (20, 2), (22, 2), (...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processor.corpus.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f1073a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric eta at 0.2\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 5 topics, 20 passes over the supplied corpus of 1740 documents, updating every 8000 documents, evaluating every ~1740 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : training LDA model using 4 processes\n",
      "INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.005*\"state\" + 0.004*\"units\" + 0.003*\"hidden\" + 0.003*\"layer\" + 0.003*\"weight\" + 0.002*\"noise\" + 0.002*\"neurons\" + 0.002*\"unit\" + 0.002*\"matrix\" + 0.002*\"distribution\"\n",
      "INFO : topic #1 (0.250): 0.003*\"units\" + 0.003*\"neurons\" + 0.002*\"image\" + 0.002*\"unit\" + 0.002*\"distribution\" + 0.002*\"layer\" + 0.002*\"state\" + 0.002*\"hidden\" + 0.002*\"recognition\" + 0.002*\"weight\"\n",
      "INFO : topic #2 (0.250): 0.004*\"units\" + 0.003*\"state\" + 0.003*\"distribution\" + 0.003*\"neuron\" + 0.003*\"neurons\" + 0.003*\"layer\" + 0.002*\"noise\" + 0.002*\"recognition\" + 0.002*\"unit\" + 0.002*\"cells\"\n",
      "INFO : topic #3 (0.250): 0.004*\"units\" + 0.003*\"hidden\" + 0.003*\"weight\" + 0.003*\"distribution\" + 0.002*\"layer\" + 0.002*\"state\" + 0.002*\"noise\" + 0.002*\"feature\" + 0.002*\"neuron\" + 0.002*\"matrix\"\n",
      "INFO : topic #4 (0.250): 0.005*\"state\" + 0.004*\"units\" + 0.003*\"hidden\" + 0.003*\"layer\" + 0.003*\"unit\" + 0.002*\"noise\" + 0.002*\"weight\" + 0.002*\"recognition\" + 0.002*\"image\" + 0.002*\"class\"\n",
      "INFO : topic diff=0.995404, rho=1.000000\n",
      "INFO : -8.518 per-word bound, 366.6 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.007*\"state\" + 0.004*\"units\" + 0.004*\"hidden\" + 0.003*\"weight\" + 0.003*\"layer\" + 0.002*\"optimal\" + 0.002*\"gradient\" + 0.002*\"states\" + 0.002*\"unit\" + 0.002*\"noise\"\n",
      "INFO : topic #1 (0.250): 0.003*\"image\" + 0.003*\"units\" + 0.003*\"neurons\" + 0.002*\"layer\" + 0.002*\"visual\" + 0.002*\"recognition\" + 0.002*\"unit\" + 0.002*\"class\" + 0.002*\"distribution\" + 0.002*\"patterns\"\n",
      "INFO : topic #2 (0.250): 0.004*\"neuron\" + 0.004*\"neurons\" + 0.003*\"cells\" + 0.003*\"cell\" + 0.003*\"units\" + 0.003*\"noise\" + 0.003*\"layer\" + 0.002*\"distribution\" + 0.002*\"circuit\" + 0.002*\"state\"\n",
      "INFO : topic #3 (0.250): 0.004*\"units\" + 0.003*\"distribution\" + 0.003*\"hidden\" + 0.003*\"weight\" + 0.003*\"gaussian\" + 0.002*\"noise\" + 0.002*\"layer\" + 0.002*\"matrix\" + 0.002*\"feature\" + 0.002*\"unit\"\n",
      "INFO : topic #4 (0.250): 0.005*\"state\" + 0.004*\"units\" + 0.004*\"hidden\" + 0.003*\"recognition\" + 0.003*\"layer\" + 0.003*\"unit\" + 0.002*\"weight\" + 0.002*\"noise\" + 0.002*\"class\" + 0.002*\"image\"\n",
      "INFO : topic diff=0.162149, rho=0.590281\n",
      "INFO : -8.436 per-word bound, 346.3 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.008*\"state\" + 0.004*\"units\" + 0.004*\"hidden\" + 0.003*\"weight\" + 0.003*\"optimal\" + 0.003*\"gradient\" + 0.003*\"algorithms\" + 0.003*\"states\" + 0.002*\"control\" + 0.002*\"distribution\"\n",
      "INFO : topic #1 (0.250): 0.005*\"image\" + 0.003*\"visual\" + 0.003*\"images\" + 0.003*\"units\" + 0.003*\"recognition\" + 0.003*\"class\" + 0.003*\"feature\" + 0.002*\"patterns\" + 0.002*\"layer\" + 0.002*\"unit\"\n",
      "INFO : topic #2 (0.250): 0.006*\"neurons\" + 0.005*\"neuron\" + 0.005*\"cells\" + 0.004*\"cell\" + 0.003*\"synaptic\" + 0.003*\"response\" + 0.003*\"circuit\" + 0.003*\"units\" + 0.003*\"noise\" + 0.003*\"current\"\n",
      "INFO : topic #3 (0.250): 0.004*\"distribution\" + 0.004*\"gaussian\" + 0.003*\"units\" + 0.003*\"noise\" + 0.003*\"matrix\" + 0.003*\"hidden\" + 0.003*\"weight\" + 0.002*\"feature\" + 0.002*\"mixture\" + 0.002*\"layer\"\n",
      "INFO : topic #4 (0.250): 0.005*\"units\" + 0.005*\"hidden\" + 0.005*\"recognition\" + 0.004*\"state\" + 0.004*\"layer\" + 0.003*\"speech\" + 0.003*\"unit\" + 0.003*\"word\" + 0.003*\"trained\" + 0.002*\"class\"\n",
      "INFO : topic diff=0.195742, rho=0.508329\n",
      "INFO : -8.365 per-word bound, 329.8 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.008*\"state\" + 0.004*\"weight\" + 0.003*\"units\" + 0.003*\"optimal\" + 0.003*\"hidden\" + 0.003*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"control\" + 0.002*\"convergence\"\n",
      "INFO : topic #1 (0.250): 0.006*\"image\" + 0.004*\"images\" + 0.004*\"visual\" + 0.003*\"feature\" + 0.003*\"recognition\" + 0.003*\"class\" + 0.003*\"units\" + 0.002*\"features\" + 0.002*\"examples\" + 0.002*\"representation\"\n",
      "INFO : topic #2 (0.250): 0.007*\"neurons\" + 0.006*\"neuron\" + 0.005*\"cells\" + 0.005*\"cell\" + 0.004*\"synaptic\" + 0.004*\"response\" + 0.003*\"activity\" + 0.003*\"circuit\" + 0.003*\"current\" + 0.003*\"firing\"\n",
      "INFO : topic #3 (0.250): 0.004*\"distribution\" + 0.004*\"gaussian\" + 0.003*\"matrix\" + 0.003*\"units\" + 0.003*\"noise\" + 0.003*\"mixture\" + 0.003*\"hidden\" + 0.003*\"density\" + 0.002*\"likelihood\" + 0.002*\"basis\"\n",
      "INFO : topic #4 (0.250): 0.006*\"units\" + 0.006*\"hidden\" + 0.006*\"recognition\" + 0.005*\"layer\" + 0.004*\"speech\" + 0.004*\"state\" + 0.003*\"unit\" + 0.003*\"word\" + 0.003*\"trained\" + 0.003*\"net\"\n",
      "INFO : topic diff=0.156095, rho=0.453143\n",
      "INFO : -8.329 per-word bound, 321.6 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.009*\"state\" + 0.004*\"weight\" + 0.004*\"optimal\" + 0.003*\"units\" + 0.003*\"algorithms\" + 0.003*\"hidden\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"control\" + 0.003*\"convergence\"\n",
      "INFO : topic #1 (0.250): 0.008*\"image\" + 0.005*\"images\" + 0.004*\"visual\" + 0.003*\"feature\" + 0.003*\"recognition\" + 0.003*\"object\" + 0.003*\"features\" + 0.003*\"class\" + 0.003*\"representation\" + 0.003*\"units\"\n",
      "INFO : topic #2 (0.250): 0.007*\"neurons\" + 0.006*\"neuron\" + 0.006*\"cells\" + 0.005*\"cell\" + 0.004*\"synaptic\" + 0.004*\"response\" + 0.004*\"activity\" + 0.003*\"circuit\" + 0.003*\"firing\" + 0.003*\"current\"\n",
      "INFO : topic #3 (0.250): 0.005*\"distribution\" + 0.005*\"gaussian\" + 0.004*\"matrix\" + 0.003*\"noise\" + 0.003*\"mixture\" + 0.003*\"units\" + 0.003*\"likelihood\" + 0.003*\"density\" + 0.003*\"hidden\" + 0.003*\"basis\"\n",
      "INFO : topic #4 (0.250): 0.007*\"units\" + 0.006*\"hidden\" + 0.006*\"recognition\" + 0.005*\"layer\" + 0.004*\"speech\" + 0.004*\"unit\" + 0.003*\"state\" + 0.003*\"word\" + 0.003*\"trained\" + 0.003*\"net\"\n",
      "INFO : topic diff=0.129925, rho=0.412744\n",
      "INFO : -8.308 per-word bound, 317.0 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.009*\"state\" + 0.004*\"optimal\" + 0.004*\"weight\" + 0.003*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"units\" + 0.003*\"states\" + 0.003*\"hidden\" + 0.003*\"control\" + 0.003*\"convergence\"\n",
      "INFO : topic #1 (0.250): 0.009*\"image\" + 0.006*\"images\" + 0.004*\"visual\" + 0.004*\"feature\" + 0.004*\"object\" + 0.004*\"recognition\" + 0.003*\"features\" + 0.003*\"representation\" + 0.003*\"class\" + 0.003*\"distance\"\n",
      "INFO : topic #2 (0.250): 0.008*\"neurons\" + 0.007*\"neuron\" + 0.006*\"cells\" + 0.005*\"cell\" + 0.004*\"synaptic\" + 0.004*\"response\" + 0.004*\"activity\" + 0.003*\"circuit\" + 0.003*\"firing\" + 0.003*\"signal\"\n",
      "INFO : topic #3 (0.250): 0.005*\"distribution\" + 0.005*\"gaussian\" + 0.004*\"matrix\" + 0.004*\"noise\" + 0.004*\"mixture\" + 0.003*\"likelihood\" + 0.003*\"density\" + 0.003*\"units\" + 0.003*\"hidden\" + 0.003*\"basis\"\n",
      "INFO : topic #4 (0.250): 0.007*\"units\" + 0.007*\"hidden\" + 0.006*\"recognition\" + 0.006*\"layer\" + 0.004*\"speech\" + 0.004*\"unit\" + 0.004*\"trained\" + 0.004*\"word\" + 0.003*\"net\" + 0.003*\"state\"\n",
      "INFO : topic diff=0.113928, rho=0.381524\n",
      "INFO : -8.294 per-word bound, 313.9 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.009*\"state\" + 0.004*\"optimal\" + 0.004*\"weight\" + 0.003*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"control\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"units\" + 0.003*\"hidden\"\n",
      "INFO : topic #1 (0.250): 0.009*\"image\" + 0.006*\"images\" + 0.005*\"visual\" + 0.004*\"object\" + 0.004*\"feature\" + 0.004*\"recognition\" + 0.004*\"features\" + 0.003*\"representation\" + 0.003*\"distance\" + 0.003*\"class\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.250): 0.008*\"neurons\" + 0.007*\"neuron\" + 0.006*\"cells\" + 0.006*\"cell\" + 0.004*\"synaptic\" + 0.004*\"response\" + 0.004*\"activity\" + 0.003*\"circuit\" + 0.003*\"signal\" + 0.003*\"firing\"\n",
      "INFO : topic #3 (0.250): 0.006*\"distribution\" + 0.005*\"gaussian\" + 0.004*\"matrix\" + 0.004*\"mixture\" + 0.004*\"noise\" + 0.004*\"likelihood\" + 0.003*\"density\" + 0.003*\"basis\" + 0.003*\"hidden\" + 0.003*\"dimensional\"\n",
      "INFO : topic #4 (0.250): 0.008*\"units\" + 0.007*\"hidden\" + 0.006*\"recognition\" + 0.006*\"layer\" + 0.005*\"speech\" + 0.004*\"unit\" + 0.004*\"trained\" + 0.004*\"word\" + 0.003*\"net\" + 0.003*\"state\"\n",
      "INFO : topic diff=0.103323, rho=0.356462\n",
      "INFO : -8.285 per-word bound, 311.8 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.010*\"state\" + 0.004*\"optimal\" + 0.004*\"weight\" + 0.003*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"control\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"generalization\" + 0.003*\"approximation\"\n",
      "INFO : topic #1 (0.250): 0.010*\"image\" + 0.007*\"images\" + 0.005*\"visual\" + 0.005*\"object\" + 0.004*\"feature\" + 0.004*\"recognition\" + 0.004*\"features\" + 0.003*\"representation\" + 0.003*\"distance\" + 0.003*\"objects\"\n",
      "INFO : topic #2 (0.250): 0.008*\"neurons\" + 0.007*\"neuron\" + 0.006*\"cells\" + 0.006*\"cell\" + 0.004*\"response\" + 0.004*\"synaptic\" + 0.004*\"activity\" + 0.004*\"circuit\" + 0.003*\"signal\" + 0.003*\"firing\"\n",
      "INFO : topic #3 (0.250): 0.006*\"distribution\" + 0.006*\"gaussian\" + 0.004*\"matrix\" + 0.004*\"mixture\" + 0.004*\"noise\" + 0.004*\"likelihood\" + 0.004*\"density\" + 0.003*\"basis\" + 0.003*\"hidden\" + 0.003*\"dimensional\"\n",
      "INFO : topic #4 (0.250): 0.008*\"units\" + 0.008*\"hidden\" + 0.006*\"recognition\" + 0.006*\"layer\" + 0.005*\"speech\" + 0.005*\"unit\" + 0.004*\"trained\" + 0.004*\"word\" + 0.004*\"net\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.095589, rho=0.335767\n",
      "INFO : -8.277 per-word bound, 310.3 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.010*\"state\" + 0.004*\"optimal\" + 0.004*\"weight\" + 0.003*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"control\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"generalization\" + 0.003*\"let\"\n",
      "INFO : topic #1 (0.250): 0.011*\"image\" + 0.007*\"images\" + 0.005*\"object\" + 0.005*\"visual\" + 0.004*\"feature\" + 0.004*\"recognition\" + 0.004*\"features\" + 0.003*\"representation\" + 0.003*\"distance\" + 0.003*\"objects\"\n",
      "INFO : topic #2 (0.250): 0.008*\"neurons\" + 0.007*\"neuron\" + 0.006*\"cells\" + 0.006*\"cell\" + 0.004*\"response\" + 0.004*\"synaptic\" + 0.004*\"activity\" + 0.004*\"circuit\" + 0.004*\"signal\" + 0.003*\"firing\"\n",
      "INFO : topic #3 (0.250): 0.006*\"distribution\" + 0.006*\"gaussian\" + 0.004*\"matrix\" + 0.004*\"mixture\" + 0.004*\"noise\" + 0.004*\"likelihood\" + 0.004*\"density\" + 0.003*\"basis\" + 0.003*\"variables\" + 0.003*\"dimensional\"\n",
      "INFO : topic #4 (0.250): 0.008*\"units\" + 0.008*\"hidden\" + 0.006*\"layer\" + 0.006*\"recognition\" + 0.005*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.089310, rho=0.318304\n",
      "INFO : -8.272 per-word bound, 309.1 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.010*\"state\" + 0.004*\"optimal\" + 0.004*\"weight\" + 0.003*\"algorithms\" + 0.003*\"control\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"generalization\" + 0.003*\"let\"\n",
      "INFO : topic #1 (0.250): 0.011*\"image\" + 0.007*\"images\" + 0.005*\"object\" + 0.005*\"visual\" + 0.005*\"feature\" + 0.004*\"recognition\" + 0.004*\"features\" + 0.003*\"representation\" + 0.003*\"distance\" + 0.003*\"objects\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.007*\"neuron\" + 0.006*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.004*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.003*\"firing\"\n",
      "INFO : topic #3 (0.250): 0.006*\"distribution\" + 0.006*\"gaussian\" + 0.004*\"matrix\" + 0.004*\"mixture\" + 0.004*\"noise\" + 0.004*\"likelihood\" + 0.004*\"density\" + 0.003*\"variables\" + 0.003*\"basis\" + 0.003*\"dimensional\"\n",
      "INFO : topic #4 (0.250): 0.009*\"units\" + 0.008*\"hidden\" + 0.006*\"layer\" + 0.006*\"recognition\" + 0.005*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.083771, rho=0.303309\n",
      "INFO : -8.267 per-word bound, 308.1 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 10, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.010*\"state\" + 0.004*\"optimal\" + 0.004*\"weight\" + 0.004*\"algorithms\" + 0.003*\"control\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"generalization\" + 0.003*\"let\"\n",
      "INFO : topic #1 (0.250): 0.011*\"image\" + 0.008*\"images\" + 0.006*\"object\" + 0.005*\"visual\" + 0.005*\"feature\" + 0.004*\"recognition\" + 0.004*\"features\" + 0.003*\"representation\" + 0.003*\"objects\" + 0.003*\"distance\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.007*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.004*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"firing\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"noise\" + 0.004*\"likelihood\" + 0.004*\"density\" + 0.003*\"variables\" + 0.003*\"basis\" + 0.003*\"dimensional\"\n",
      "INFO : topic #4 (0.250): 0.009*\"units\" + 0.008*\"hidden\" + 0.007*\"layer\" + 0.006*\"recognition\" + 0.005*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.078680, rho=0.290252\n",
      "INFO : -8.264 per-word bound, 307.4 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 11, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.010*\"state\" + 0.004*\"optimal\" + 0.004*\"weight\" + 0.004*\"algorithms\" + 0.003*\"control\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"generalization\" + 0.003*\"let\"\n",
      "INFO : topic #1 (0.250): 0.012*\"image\" + 0.008*\"images\" + 0.006*\"object\" + 0.005*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.004*\"features\" + 0.004*\"objects\" + 0.004*\"representation\" + 0.004*\"distance\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.007*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"likelihood\" + 0.004*\"noise\" + 0.004*\"density\" + 0.003*\"variables\" + 0.003*\"basis\" + 0.003*\"log\"\n",
      "INFO : topic #4 (0.250): 0.009*\"units\" + 0.008*\"hidden\" + 0.007*\"layer\" + 0.006*\"recognition\" + 0.005*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.073879, rho=0.278747\n",
      "INFO : -8.261 per-word bound, 306.8 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 12, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.010*\"state\" + 0.004*\"optimal\" + 0.004*\"weight\" + 0.004*\"algorithms\" + 0.004*\"control\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"generalization\" + 0.003*\"let\"\n",
      "INFO : topic #1 (0.250): 0.012*\"image\" + 0.008*\"images\" + 0.006*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.004*\"features\" + 0.004*\"objects\" + 0.004*\"representation\" + 0.004*\"distance\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"likelihood\" + 0.004*\"noise\" + 0.004*\"density\" + 0.003*\"variables\" + 0.003*\"basis\" + 0.003*\"log\"\n",
      "INFO : topic #4 (0.250): 0.010*\"units\" + 0.008*\"hidden\" + 0.007*\"layer\" + 0.006*\"recognition\" + 0.006*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.069310, rho=0.268511\n",
      "INFO : -8.259 per-word bound, 306.3 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 13, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.010*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"\n",
      "INFO : topic #1 (0.250): 0.012*\"image\" + 0.008*\"images\" + 0.006*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"representation\" + 0.004*\"distance\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"mixture\" + 0.004*\"noise\" + 0.004*\"density\" + 0.003*\"variables\" + 0.003*\"basis\" + 0.003*\"log\"\n",
      "INFO : topic #4 (0.250): 0.010*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"recognition\" + 0.006*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.064961, rho=0.259325\n",
      "INFO : -8.257 per-word bound, 305.9 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 14, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.010*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"\n",
      "INFO : topic #1 (0.250): 0.012*\"image\" + 0.008*\"images\" + 0.006*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"representation\" + 0.004*\"distance\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"noise\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"variables\" + 0.003*\"log\" + 0.003*\"basis\"\n",
      "INFO : topic #4 (0.250): 0.010*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"recognition\" + 0.006*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.060849, rho=0.251022\n",
      "INFO : -8.255 per-word bound, 305.5 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 15, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.011*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"\n",
      "INFO : topic #1 (0.250): 0.013*\"image\" + 0.008*\"images\" + 0.006*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"representation\" + 0.004*\"distance\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"noise\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"variables\" + 0.003*\"log\" + 0.003*\"basis\"\n",
      "INFO : topic #4 (0.250): 0.010*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"recognition\" + 0.006*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.056989, rho=0.243468\n",
      "INFO : -8.254 per-word bound, 305.2 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 16, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.011*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"\n",
      "INFO : topic #1 (0.250): 0.013*\"image\" + 0.008*\"images\" + 0.007*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"representation\" + 0.004*\"distance\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"noise\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"variables\" + 0.003*\"log\" + 0.003*\"variance\"\n",
      "INFO : topic #4 (0.250): 0.010*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"recognition\" + 0.006*\"unit\" + 0.005*\"speech\" + 0.004*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.003*\"patterns\"\n",
      "INFO : topic diff=0.053396, rho=0.236558\n",
      "INFO : -8.252 per-word bound, 304.9 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 17, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.011*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"\n",
      "INFO : topic #1 (0.250): 0.013*\"image\" + 0.009*\"images\" + 0.007*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"representation\" + 0.004*\"position\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"noise\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"variables\" + 0.003*\"log\" + 0.003*\"variance\"\n",
      "INFO : topic #4 (0.250): 0.010*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"unit\" + 0.006*\"recognition\" + 0.005*\"speech\" + 0.005*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.004*\"patterns\"\n",
      "INFO : topic diff=0.050058, rho=0.230205\n",
      "INFO : -8.251 per-word bound, 304.7 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 18, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.011*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"gradient\" + 0.003*\"states\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"\n",
      "INFO : topic #1 (0.250): 0.013*\"image\" + 0.009*\"images\" + 0.007*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"position\" + 0.004*\"representation\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"noise\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"variables\" + 0.003*\"log\" + 0.003*\"variance\"\n",
      "INFO : topic #4 (0.250): 0.010*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"unit\" + 0.006*\"recognition\" + 0.005*\"speech\" + 0.005*\"trained\" + 0.004*\"net\" + 0.004*\"word\" + 0.004*\"patterns\"\n",
      "INFO : topic diff=0.046973, rho=0.224337\n",
      "INFO : -8.250 per-word bound, 304.5 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : PROGRESS: pass 19, dispatched chunk #0 = documents up to #1740/1740, outstanding queue size 1\n",
      "INFO : topic #0 (0.250): 0.011*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"states\" + 0.003*\"gradient\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"\n",
      "INFO : topic #1 (0.250): 0.013*\"image\" + 0.009*\"images\" + 0.007*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"position\" + 0.004*\"representation\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"noise\" + 0.004*\"likelihood\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"variables\" + 0.003*\"log\" + 0.003*\"variance\"\n",
      "INFO : topic #4 (0.250): 0.011*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"unit\" + 0.006*\"recognition\" + 0.005*\"speech\" + 0.005*\"trained\" + 0.004*\"net\" + 0.004*\"patterns\" + 0.004*\"word\"\n",
      "INFO : topic diff=0.044130, rho=0.218896\n",
      "INFO : -8.249 per-word bound, 304.3 perplexity estimate based on a held-out corpus of 1740 documents with 2015059 words\n",
      "INFO : LdaMulticore lifecycle event {'msg': 'trained LdaMulticore<num_terms=11883, num_topics=5, decay=0.5, chunksize=2000> in 622.83s', 'datetime': '2022-12-16T12:30:12.731954', 'gensim': '4.2.0', 'python': '3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "word_dict[0]\n",
    "baseline_lda_model = LdaMulticore(\n",
    "    corpus = data_processor.corpus[\"embeddings\"].tolist(),\n",
    "    id2word=word_dict.id2token,\n",
    "    workers=4,\n",
    "    alpha=0.25,\n",
    "    num_topics=5,\n",
    "    minimum_probability=0.2,\n",
    "    passes=20,\n",
    "    per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3822f91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.250): 0.011*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"states\" + 0.003*\"gradient\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"\n",
      "INFO : topic #1 (0.250): 0.013*\"image\" + 0.009*\"images\" + 0.007*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"position\" + 0.004*\"representation\"\n",
      "INFO : topic #2 (0.250): 0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"\n",
      "INFO : topic #3 (0.250): 0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"noise\" + 0.004*\"likelihood\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"variables\" + 0.003*\"log\" + 0.003*\"variance\"\n",
      "INFO : topic #4 (0.250): 0.011*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"unit\" + 0.006*\"recognition\" + 0.005*\"speech\" + 0.005*\"trained\" + 0.004*\"net\" + 0.004*\"patterns\" + 0.004*\"word\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"state\" + 0.005*\"optimal\" + 0.004*\"weight\" + 0.004*\"control\" + 0.004*\"algorithms\" + 0.003*\"states\" + 0.003*\"gradient\" + 0.003*\"convergence\" + 0.003*\"let\" + 0.003*\"generalization\"'),\n",
       " (1,\n",
       "  '0.013*\"image\" + 0.009*\"images\" + 0.007*\"object\" + 0.006*\"visual\" + 0.005*\"feature\" + 0.005*\"recognition\" + 0.005*\"features\" + 0.004*\"objects\" + 0.004*\"position\" + 0.004*\"representation\"'),\n",
       " (2,\n",
       "  '0.009*\"neurons\" + 0.008*\"neuron\" + 0.007*\"cells\" + 0.006*\"cell\" + 0.005*\"response\" + 0.005*\"synaptic\" + 0.005*\"activity\" + 0.004*\"signal\" + 0.004*\"circuit\" + 0.004*\"frequency\"'),\n",
       " (3,\n",
       "  '0.007*\"distribution\" + 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"noise\" + 0.004*\"likelihood\" + 0.004*\"mixture\" + 0.004*\"density\" + 0.004*\"variables\" + 0.003*\"log\" + 0.003*\"variance\"'),\n",
       " (4,\n",
       "  '0.011*\"units\" + 0.009*\"hidden\" + 0.007*\"layer\" + 0.006*\"unit\" + 0.006*\"recognition\" + 0.005*\"speech\" + 0.005*\"trained\" + 0.004*\"net\" + 0.004*\"patterns\" + 0.004*\"word\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab3356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6f92685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using ParallelWordOccurrenceAccumulator<processes=7, batch_size=64> to estimate probabilities from sliding windows\n",
      "INFO : 1 batches submitted to accumulate stats from 64 documents (309434 virtual)\n",
      "INFO : 2 batches submitted to accumulate stats from 128 documents (600331 virtual)\n",
      "INFO : 3 batches submitted to accumulate stats from 192 documents (841782 virtual)\n",
      "INFO : 4 batches submitted to accumulate stats from 256 documents (1103909 virtual)\n",
      "INFO : 5 batches submitted to accumulate stats from 320 documents (1327582 virtual)\n",
      "INFO : 6 batches submitted to accumulate stats from 384 documents (1568367 virtual)\n",
      "INFO : 7 batches submitted to accumulate stats from 448 documents (1802147 virtual)\n",
      "INFO : 8 batches submitted to accumulate stats from 512 documents (2065111 virtual)\n",
      "INFO : 9 batches submitted to accumulate stats from 576 documents (2310387 virtual)\n",
      "INFO : 10 batches submitted to accumulate stats from 640 documents (2574365 virtual)\n",
      "INFO : 11 batches submitted to accumulate stats from 704 documents (2828896 virtual)\n",
      "INFO : 12 batches submitted to accumulate stats from 768 documents (3081354 virtual)\n",
      "INFO : 13 batches submitted to accumulate stats from 832 documents (3329177 virtual)\n",
      "INFO : 14 batches submitted to accumulate stats from 896 documents (3602743 virtual)\n",
      "INFO : 15 batches submitted to accumulate stats from 960 documents (3867738 virtual)\n",
      "INFO : 16 batches submitted to accumulate stats from 1024 documents (4133225 virtual)\n",
      "INFO : 17 batches submitted to accumulate stats from 1088 documents (4397623 virtual)\n",
      "INFO : 18 batches submitted to accumulate stats from 1152 documents (4663389 virtual)\n",
      "INFO : 19 batches submitted to accumulate stats from 1216 documents (4913138 virtual)\n",
      "INFO : 20 batches submitted to accumulate stats from 1280 documents (5174353 virtual)\n",
      "INFO : 21 batches submitted to accumulate stats from 1344 documents (5447553 virtual)\n",
      "INFO : 22 batches submitted to accumulate stats from 1408 documents (5705459 virtual)\n",
      "INFO : 23 batches submitted to accumulate stats from 1472 documents (5979319 virtual)\n",
      "INFO : 24 batches submitted to accumulate stats from 1536 documents (6235992 virtual)\n",
      "INFO : 25 batches submitted to accumulate stats from 1600 documents (6520145 virtual)\n",
      "INFO : 26 batches submitted to accumulate stats from 1664 documents (6790783 virtual)\n",
      "INFO : 27 batches submitted to accumulate stats from 1728 documents (7072050 virtual)\n",
      "INFO : 28 batches submitted to accumulate stats from 1792 documents (7128147 virtual)\n",
      "INFO : 7 accumulators retrieved from output queue\n",
      "INFO : accumulated word occurrence stats for 7128147 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.8198481147665981\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=baseline_lda_model, texts=data_processor.corpus.tokens.tolist(), dictionary=word_dict, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57a16969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b999545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec_lda(model, corpus, k):\n",
    "    \"\"\"\n",
    "    Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
    "    :return: vec_lda with dimension: (n_doc * n_topic)\n",
    "    \"\"\"\n",
    "    n_doc = len(corpus)\n",
    "    vec_lda = np.zeros((n_doc, k))\n",
    "    for i in range(n_doc):\n",
    "        # get the distribution for the i-th document in corpus\n",
    "        for topic, prob in model.get_document_topics(corpus[i]):\n",
    "            vec_lda[i, topic] = prob\n",
    "    return vec_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d6acb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = get_vec_lda(baseline_lda_model, data_processor.corpus.embeddings.tolist(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc0c5197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1740, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "066ba969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a645756b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Load pretrained SentenceTransformer: bert-base-nli-max-tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bc44c03ea74e0290acbff38979c30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d382aeab9d5b4b1d888ff5555209167e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d664f2a1d74d0fbfc452ace5195349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b00f9351afc46d1bbc62bd74365b0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e136721137541419a5000aff6b7ae57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaafe2e4eece49c38834e52a07d4d9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c1ec2ae854405ab9c745d012ecdaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a07dd19b2d94652bea098bfb19bbe82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22a838f0476468f98d9bfa4e4a33609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15254b3fe90942a2b3a40daf5127e466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9ef966fdfd46528ce521c6fc6a9fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/397 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c9e944f4c04a9b90556df476e025d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc8b613c9184746b7620f9c61e01a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Use pytorch device: cuda\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-nli-max-tokens\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(model\u001b[38;5;241m.\u001b[39mencode(\u001b[43msentences\u001b[49m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('bert-base-nli-max-tokens')\n",
    "vec = np.array(model.encode(sentences, show_progress_bar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea53e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
