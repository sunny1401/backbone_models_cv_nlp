from src.cv.model_pipelines.dl_base_pipeline import CNNTrainingPipeline
from src.cv.pytorch.models.use_cases.facial_keypoint_vanilla_cnn import FacialKeypointVCNN
from typing import Dict, List, Optional
import numpy as np
from torch.utils.data import Dataset
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch


class FacialCNNTrainingPipeline(CNNTrainingPipeline):
    # Doctring generated by chatGPT
    """
    A pipeline to train a FacialKeypointVCNN model on a facial keypoint detection dataset.
    Parameters:
        dataset (Dataset): The dataset to be used for training and validation.
        model_training_config (Dict): A dictionary containing the training configuration such as number of epochs, batch size etc.
        model_data_config (Dict): A dictionary containing the data configuration such as data preprocessing options.
        model_initialization_params (Dict): A dictionary containing the parameters to initialize the model.
        load_model_from_path (str, optional): The path to the checkpoint file containing a pre-trained model to be loaded.

    Methods:
        initialize_optimization_parameters(lr: float) -> Dict: Initializes the optimizer and criterion to be used for training.
        _initialize_model(device: str, model_params: Dict) -> torch.nn.Module: Initializes the model with the given parameters.
        _fit_model() -> float: Trains the model on the training data and returns the training loss.
        _validate_model() -> float: Validates the model on the validation data and returns the validation loss.
        get_predictions(test_dataloader: DataLoader, device: str) -> List: Makes predictions on the test data and returns a list of predictions.
    """

    def __init__(self, 
        dataset: Dataset, 
        model_training_config: Dict, 
        model_data_config: Dict, 
        model_initialization_params: Dict,
        load_model_from_path: Optional[str] = None
    ):
          super().__init__(
            dataset=dataset, 
            model_training_config=model_training_config, 
            model_data_config=model_data_config, 
            model_initialization_params=model_initialization_params,
            load_model_from_path=load_model_from_path
        )

    def initialize_optimization_parameters(self, lr=0.0005) -> Dict:
        criterion = nn.MSELoss()
        optimizer = optim.Adam(
            self.model.parameters(), lr=lr
        )
        
        return optimizer, criterion

    def _initialize_model(self, device, model_params):
        model = FacialKeypointVCNN(**model_params).to(device)
        return model

    def _fit_model(self):

        self.model.train()
        batch_training_loss: float = 0
        
        for idx, data in enumerate(self._train_dataloader):
            image = data["image"]
            keypoints = data["facial_landmarks"]
            # flatten pts
            keypoints = keypoints.view(keypoints.size(0), -1)
            # convert variables to floats for regression loss
            keypoints = keypoints.type(torch.FloatTensor)
            image = image.type(torch.FloatTensor)
            # inp is shape (N, C, H, W)
            image = image.reshape(image.shape[0], image.shape[-1], image.shape[1], image.shape[2])
            image = image.to(self.model_training_config.device)
            keypoints = keypoints.to(self.model_training_config.device)
            self.optimizer.zero_grad()
            outputs = self.model(image)
            loss = self.criterion(
                outputs, 
                keypoints
            )
            batch_training_loss += loss.item()
            loss.backward()
            self.optimizer.step()
        
        train_loss = batch_training_loss/ idx +1
        return train_loss

    def _validate_model(self):
        batch_validation_loss: float = 0
        self.model.eval()
        with torch.no_grad():

            for idx, data in enumerate(self._validation_dataloader):
                image = data["image"]
                keypoints = data["facial_landmarks"]
                # flatten pts
                keypoints = keypoints.view(keypoints.size(0), -1)
                # convert variables to floats for regression loss
                keypoints = keypoints.type(torch.FloatTensor)
                image = image.type(torch.FloatTensor)
                 # inp is shape (N, C, H, W)
                image = image.reshape(image.shape[0], image.shape[-1], image.shape[1], image.shape[2])
                image = image.to(self.model_training_config.device)
                keypoints = keypoints.to(self.model_training_config.device)
                outputs = self.model(image)
                loss = self.criterion(
                    outputs, 
                    keypoints
                )
                batch_validation_loss += loss.item()

        validation_loss = batch_validation_loss/idx + 1
        if self._min_validation_loss > validation_loss:
            self._min_validation_loss = validation_loss
            self._final_trained_model = self.model
        return validation_loss

    def get_predictions(self, test_dataloader: List) -> List:

        model = self.best_model
        model.eval()

        output_keyp = []
        for data in test_dataloader:
            image = data.type(torch.FloatTensor)
            # inp is shape (N, C, H, W)
            image = image.reshape(image.shape[0], image.shape[-1], image.shape[1], image.shape[2])
            image = image.to(self.model_training_config.device)
            outputs = model(image)
            if self.model_training_config.device == "cuda":
                outputs = outputs.cpu().data.numpy()
            output_keyp.append(outputs)

        return [preds.cpu().detach().numpy() for batch in output_keyp for preds in batch]

    def generate_test_dataloader_from_dataset(self, dataset: Dataset) -> List:

        dataloader = []
        index = 0
        dataset_finished = False
        current_end = self.model_training_config.batch_size
        while(current_end <= len(dataset)):
            dataloader.append(torch.from_numpy(np.array(
                [dataset[j]["image"] for j in range(index, current_end)]
            )))
            if dataset_finished:
                break
            index = current_end
            current_end += self.model_training_config.batch_size
            if current_end > len(dataset):
                current_end = len(dataset)
                dataset_finished = True

        return dataloader